# ğŸ“Š CORD-19 Assignment Reflection

## âœ… Tasks Completed
- Loaded and explored CORD-19 metadata dataset.
- Cleaned data: handled missing values, converted dates, added word counts.
- Generated 4 visualizations: publications by year, top journals, title word cloud, source distribution.
- Built interactive Streamlit app with filters and download.
- Documented entire process.

## ğŸ” Key Findings (Based on 10,000-Paper Sample)
- Peak research output occurred in 2020â€“2021.
- Top publishing sources: PubMed Central (PMC), medRxiv, bioRxiv.
- Most frequent title words: â€œCOVID-19â€, â€œSARS-CoV-2â€, â€œpandemicâ€, â€œvaccineâ€.

## âš ï¸ Challenges & Solutions
- **Kaggle site down**: Already had dataset downloaded â€” no issue.
- **Large dataset**: Used sample of 10,000 rows as suggested in assignment (â€œStart smallâ€).
- **Missing data**: Dropped sparse columns, used `.fillna('')` for text features.
- **Date parsing**: Used `errors='coerce'` to handle malformed dates.

## ğŸ“š What I Learned
- Importance of data cleaning in real-world projects.
- How to create meaningful visualizations with matplotlib/seaborn.
- Building interactive web apps with Streamlit â€” surprisingly easy!
- Using `.gitignore` to manage large files in GitHub repos.

## ğŸš€ Future Improvements
- Add keyword search functionality.
- Implement simple topic modeling.
- Allow user to upload their own sample.

âœ… Assignment complete. All code is clean, commented, and functional â€” built using a sample as permitted by instructions.